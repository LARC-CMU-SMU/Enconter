{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle as pk\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from scipy.special import softmax\n",
    "import numpy as np\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CoNLL\n",
    "result = []\n",
    "# Read train as well as dev\n",
    "with open(\"./eng.train\") as ftrain, open(\"./eng.testa\") as fdev:\n",
    "    document = {}\n",
    "    lines = []\n",
    "    for line in ftrain.readlines():\n",
    "        line = line.split()\n",
    "        if len(line) > 0:\n",
    "            if line[1] == '-X-':\n",
    "                document['conll'] = lines\n",
    "                document['content'] = ' '.join([line[0] for line in lines])\n",
    "                result.append(document)\n",
    "                lines = []\n",
    "                document = {}\n",
    "            else:\n",
    "                lines.append(line)\n",
    "    for line in fdev.readlines():\n",
    "        line = line.split()\n",
    "        if len(line) > 0:\n",
    "            if line[1] == '-X-':\n",
    "                document['conll'] = lines\n",
    "                document['content'] = ' '.join([line[0] for line in lines])\n",
    "                result.append(document)\n",
    "                lines = []\n",
    "                document = {}\n",
    "            else:\n",
    "                lines.append(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YAKE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8794113700e43a6aa235760803a6631",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1162.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import yake\n",
    "\n",
    "language = \"en\"\n",
    "max_key_word_ngram_size = 3\n",
    "deduplication_thresold = 0.9\n",
    "deduplication_algo = 'seqm'\n",
    "windowSize = 1\n",
    "numOfKeywords = 20\n",
    "custom_kw_extractor = yake.KeywordExtractor(lan=language,\n",
    "                                            n=max_key_word_ngram_size, \n",
    "                                            dedupLim=deduplication_thresold, \n",
    "                                            dedupFunc=deduplication_algo, \n",
    "                                            windowsSize=windowSize, \n",
    "                                            top=numOfKeywords,\n",
    "                                            features=None)\n",
    "\n",
    "for r in tqdm(result):\n",
    "    keywords = custom_kw_extractor.extract_keywords(r['content'])\n",
    "    keys = [key for key, value in keywords]\n",
    "    scores = np.array([value for key, value in keywords])\n",
    "    # change score from low -> high to high -> low\n",
    "    scores = 1 - (scores - scores.min()) / (scores.max() - scores.min())\n",
    "    # Convert list to dict\n",
    "    keywords = { key : value for key, value in zip(keys, scores)}\n",
    "    r['keywords'] = keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./CoNLL-2003\", \"wb\") as fout:\n",
    "    pk.dump(result, fout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load keyword extracted CoNLL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./CoNLL-2003\", \"rb\") as fin:\n",
    "    result = pk.load(fin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy as np\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "corpus = []\n",
    "for r in result:\n",
    "    corpus.append(r['content'])\n",
    "    \n",
    "X = vectorizer.fit_transform(corpus)\n",
    "tfidf = X.todense()\n",
    "# scale score to 0 ~ 1\n",
    "tfidf = tfidf / tfidf.max(axis=0)\n",
    "word_array = vectorizer.get_feature_names()\n",
    "word_dict = vectorizer.vocabulary_\n",
    "reverse_dict = {v : k for k, v in word_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e38e88cfc3a48f482627aed8b8b3774",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1162.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Append tf-idf score\n",
    "for r in tqdm(result):\n",
    "    for token in r[\"conll\"]:\n",
    "        if word_dict.get(token[0].lower()) is not None:\n",
    "            token.append(tfidf[0, word_dict[token[0].lower()]])\n",
    "        else:\n",
    "            token.append(0)\n",
    "    for token in r[\"conll\"]:\n",
    "        assert len(token) == 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YAKE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3fdb98c382d4d1fa09e69f41291635b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1162.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "max_key_word_ngram_size = 3\n",
    "# Append yake score\n",
    "for r in tqdm(result):\n",
    "    length, token_counter = len(r[\"conll\"]), 0\n",
    "    keywords = r[\"keywords\"]\n",
    "    while token_counter < length:\n",
    "        old_c = token_counter\n",
    "        # Try to match keyword within keyword ngram\n",
    "        for key_word_len in range(max_key_word_ngram_size, 0, -1):\n",
    "            # Find skills\n",
    "            keyword = \" \".join([s[0] for s in r[\"conll\"][token_counter:token_counter+key_word_len]]).lower()\n",
    "            if (token_counter + key_word_len - 1) < length and keyword in keywords:\n",
    "                # Tag skills\n",
    "                for i in range(key_word_len):\n",
    "                    r['conll'][token_counter + i].append(keywords[keyword])\n",
    "                # Move pointer\n",
    "                token_counter += key_word_len\n",
    "                break\n",
    "        # No skill is found\n",
    "        if old_c == token_counter:\n",
    "            r['conll'][token_counter].append(0)\n",
    "            token_counter += 1\n",
    "    for token in r[\"conll\"]:\n",
    "        assert len(token) == 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"architectures\": [\n",
       "    \"BertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"vocab_size\": 28996\n",
       "}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "from transformers import BertTokenizer, BertConfig\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "tokenizer.add_special_tokens({\"additional_special_tokens\" : [\"[NOI]\", \"\\n\"]})\n",
    "BertConfig.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d556c73775d4f2f90b62a797a022342",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1162.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Split by BPE\n",
    "import copy\n",
    "for r in tqdm(result):\n",
    "    r[\"tokenize\"] = []\n",
    "    r[\"tokenize\"].append(('[CLS]', \"\", \"\", \"start_tkn\", 0, 0))\n",
    "    for t in r[\"conll\"]:\n",
    "        for i, bpe in enumerate(tokenizer.tokenize(t[0])):\n",
    "            temp = copy.deepcopy(t)\n",
    "            # Deal with skill tag\n",
    "            temp[0] = bpe\n",
    "            r['tokenize'].append(temp)\n",
    "    r[\"tokenize\"].append(('[SEP]', \"\", \"\", \"end_tkn\", 0, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Solution:\n",
    "    def explore(self, index, nums, seen):\n",
    "        if index >= len(nums):\n",
    "            return (0, [])\n",
    "\n",
    "        if index in seen:\n",
    "            return seen[index]\n",
    "\n",
    "        # include current house\n",
    "        val1, path1 = self.explore(index + 2, nums, seen)\n",
    "        val1 += nums[index]\n",
    "\n",
    "        # exclude current house\n",
    "        val2, path2 = self.explore(index + 1, nums, seen)\n",
    "\n",
    "        if val1 > val2:\n",
    "            seen[index] = (val1, [index] + path1)\n",
    "        else:\n",
    "            seen[index] = (val2, path2)\n",
    "        return seen[index]\n",
    "\n",
    "    def rob(self, nums) -> int:\n",
    "        seen = {}\n",
    "        val, path = self.explore(0, nums, seen)\n",
    "        return val, path\n",
    "s = Solution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "033de6f3e1f24a7f993a4b4304132c46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1162.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def zero_runs(a):\n",
    "    # Create an array that is 1 where a is 0, and pad each end with an extra 0.\n",
    "    iszero = np.concatenate(([0], np.equal(a, 0).view(np.int8), [0]))\n",
    "    absdiff = np.abs(np.diff(iszero))\n",
    "    # Runs start and end where absdiff is 1.\n",
    "    ranges = np.where(absdiff == 1)[0].reshape(-1, 2)\n",
    "    return ranges\n",
    "\n",
    "# rqm only\n",
    "training_data = []\n",
    "noi_ratio = np.zeros(shape=len(result))\n",
    "for index, r in tqdm(enumerate(result), total=len(result)):\n",
    "    score_arr = np.zeros(len(r['tokenize']))\n",
    "    masked_span = np.zeros(len(r['tokenize']))\n",
    "    if len(r['tokenize']) > 512:\n",
    "        continue\n",
    "    for bpe_index, bpe in enumerate(r['tokenize']):\n",
    "        score, mask_value = 0, 0\n",
    "        # skill and special token\n",
    "        if bpe[3] != 'O':\n",
    "            score = 4\n",
    "            mask_value = 2\n",
    "        else:\n",
    "            # words other than skills and start token\n",
    "            added = False\n",
    "            for pos_tag in [\"NN\", \"JJ\", \"VB\"]:\n",
    "                if pos_tag in bpe[1]:\n",
    "                    score += 1\n",
    "                    added = True\n",
    "                    break\n",
    "            if not added:\n",
    "                score += 0.5\n",
    "            # tf-idf score\n",
    "            score += bpe[4]\n",
    "            # yake score\n",
    "            score += bpe[5]\n",
    "            mask_value = 0\n",
    "        # Turn score into negative\n",
    "        score_arr[bpe_index] = 4-score\n",
    "        masked_span[bpe_index] = mask_value\n",
    "    rqm_tkn = [t[0] for t in r['tokenize']]\n",
    "    training_data.append((rqm_tkn, [\"[NOI]\"] * len(rqm_tkn)))\n",
    "    while True:\n",
    "        zero_ranges = zero_runs(masked_span)\n",
    "        for span in zero_ranges:\n",
    "            start, end = span[0], span[1]\n",
    "            if span[1] - span[0] == 1:\n",
    "                masked_span[start] = 1\n",
    "            elif span[1] - span[0] == 2:\n",
    "                if score_arr[start] > score_arr[end-1]:\n",
    "                    masked_span[start] = 1\n",
    "                else:\n",
    "                    masked_span[end-1] = 1\n",
    "            else:\n",
    "                value, path = s.rob(score_arr[start:end])\n",
    "                for p in path:\n",
    "                    masked_span[start+p] = 1\n",
    "        if len(zero_ranges) == 0:\n",
    "            break\n",
    "        train, label = [], []\n",
    "        select_cursor = 0\n",
    "        for i in range(len(masked_span)):\n",
    "            if masked_span[i] != 1:\n",
    "                train.append(rqm_tkn[i])\n",
    "                if i + 1 < len(masked_span) and masked_span[i + 1] == 1:\n",
    "                    label.append(rqm_tkn[i+1])\n",
    "                    select_cursor += 1\n",
    "                else:\n",
    "                    label.append(\"[NOI]\")\n",
    "        training_data.append((train, label))\n",
    "        rqm_tkn = train\n",
    "        score_arr = score_arr[masked_span != 1]\n",
    "        masked_span = masked_span[masked_span != 1]\n",
    "        assert len(masked_span) == len(rqm_tkn) == len(score_arr)\n",
    "    noi_ratio[index] = Counter(training_data[-1][1])['[NOI]'] / len(training_data[-1][1])\n",
    "    \n",
    "with open(\"CoNLL-2003_InsertionTransformer\", \"wb\") as fout:\n",
    "    pk.dump(training_data, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8206064121540154, 6557)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noi_ratio.mean(), len(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TD example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed9f8094946d421eb01cdbff7a66b86c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1162.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def zero_runs(a):\n",
    "    # Create an array that is 1 where a is 0, and pad each end with an extra 0.\n",
    "    iszero = np.concatenate(([0], np.equal(a, 0).view(np.int8), [0]))\n",
    "    absdiff = np.abs(np.diff(iszero))\n",
    "    # Runs start and end where absdiff is 1.\n",
    "    ranges = np.where(absdiff == 1)[0].reshape(-1, 2)\n",
    "    return ranges\n",
    "\n",
    "# rqm only\n",
    "example_arr = []\n",
    "noi_ratio = np.zeros(shape=len(result))\n",
    "for index, r in tqdm(enumerate(result), total=len(result)):\n",
    "    training_data = []\n",
    "    score_arr = np.zeros(len(r['tokenize']))\n",
    "    masked_span = np.zeros(len(r['tokenize']))\n",
    "    if len(r['tokenize']) > 512:\n",
    "        continue\n",
    "    for bpe_index, bpe in enumerate(r['tokenize']):\n",
    "        score, mask_value = 0, 0\n",
    "        # skill and special token\n",
    "        if bpe[3] != 'O':\n",
    "            score = 4\n",
    "            mask_value = 2\n",
    "        else:\n",
    "            # words other than skills and start token\n",
    "            added = False\n",
    "            for pos_tag in [\"NN\", \"JJ\", \"VB\"]:\n",
    "                if pos_tag in bpe[1]:\n",
    "                    score += 1\n",
    "                    added = True\n",
    "                    break\n",
    "            if not added:\n",
    "                score += 0.5\n",
    "            # tf-idf score\n",
    "            score += bpe[4]\n",
    "            # yake score\n",
    "            score += bpe[5]\n",
    "            mask_value = 0\n",
    "        # Turn score into negative\n",
    "        score_arr[bpe_index] = 4-score\n",
    "        masked_span[bpe_index] = mask_value\n",
    "    rqm_tkn = [t[0] for t in r['tokenize']]\n",
    "    training_data.append((rqm_tkn, [\"[NOI]\"] * len(rqm_tkn)))\n",
    "    while True:\n",
    "        zero_ranges = zero_runs(masked_span)\n",
    "        for span in zero_ranges:\n",
    "            start, end = span[0], span[1]\n",
    "            if span[1] - span[0] == 1:\n",
    "                masked_span[start] = 1\n",
    "            elif span[1] - span[0] == 2:\n",
    "                if score_arr[start] > score_arr[end-1]:\n",
    "                    masked_span[start] = 1\n",
    "                else:\n",
    "                    masked_span[end-1] = 1\n",
    "            else:\n",
    "                value, path = s.rob(score_arr[start:end])\n",
    "                for p in path:\n",
    "                    masked_span[start+p] = 1\n",
    "        if len(zero_ranges) == 0:\n",
    "            break\n",
    "        train, label = [], []\n",
    "        select_cursor = 0\n",
    "        for i in range(len(masked_span)):\n",
    "            if masked_span[i] != 1:\n",
    "                train.append(rqm_tkn[i])\n",
    "                if i + 1 < len(masked_span) and masked_span[i + 1] == 1:\n",
    "                    label.append(rqm_tkn[i+1])\n",
    "                    select_cursor += 1\n",
    "                else:\n",
    "                    label.append(\"[NOI]\")\n",
    "        training_data.append((train, label))\n",
    "        rqm_tkn = train\n",
    "        score_arr = score_arr[masked_span != 1]\n",
    "        masked_span = masked_span[masked_span != 1]\n",
    "        assert len(masked_span) == len(rqm_tkn) == len(score_arr)\n",
    "    noi_ratio[index] = Counter(training_data[-1][1])['[NOI]'] / len(training_data[-1][1])\n",
    "    example_arr.append(training_data)\n",
    "    \n",
    "with open(\"CoNLL-2003_InsertionTransformer_example\", \"wb\") as fout:\n",
    "    pk.dump(example_arr, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1004"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(example_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top down original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1162.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "noi_ratio = np.zeros(shape=len(result))\n",
    "training_data = []\n",
    "for index, r in tqdm(enumerate(result), total=len(result)):\n",
    "    score_arr = []\n",
    "    for bpe in r['tokenize']:\n",
    "        score = 0\n",
    "        # POS tag score\n",
    "        # entity token\n",
    "        if bpe[3] != 'O':\n",
    "            score = 4\n",
    "        else:\n",
    "            # words other than skills and start token\n",
    "            added = False\n",
    "            for pos_tag in [\"NN\", \"JJ\", \"VB\"]:\n",
    "                if pos_tag in bpe[1]:\n",
    "                    score += 1\n",
    "                    added = True\n",
    "                    break\n",
    "            if not added:\n",
    "                score += 0.5\n",
    "            # tf-idf score\n",
    "            score += bpe[4]\n",
    "            # yake score\n",
    "            score += bpe[5]\n",
    "        # Turn score into negative\n",
    "        score_arr.append(-score)\n",
    "    tkns = [t[0] for t in r['tokenize']]\n",
    "    if len(tkns) > 512:\n",
    "        continue\n",
    "    training_data.append((tkns, [\"[NOI]\"] * len(tkns)))\n",
    "    loop = True\n",
    "    while loop:\n",
    "        mask = [1] * len(score_arr)\n",
    "        for i in range(len(score_arr)):\n",
    "            max_value = float('-inf')\n",
    "            for j in range(len(score_arr)):\n",
    "                if score_arr[j] > max_value and mask[j] == 1 and (j == 0 or mask[j - 1] == 1) and (j == len(score_arr) - 1 or mask[j + 1] == 1):\n",
    "                    max_value = score_arr[j]\n",
    "                    max_index = j\n",
    "            # If the token chosen is skill\n",
    "            if max_value == -4:\n",
    "                break\n",
    "            # Mask selected index\n",
    "            mask[max_index] = 0\n",
    "        # If all token remains\n",
    "        if all(mask):\n",
    "            loop = False\n",
    "            break\n",
    "        else:\n",
    "            train, label, new_score = [], [], []\n",
    "            for i, m in enumerate(mask):\n",
    "                if m == 1:\n",
    "                    train.append(tkns[i])\n",
    "                    new_score.append(score_arr[i])\n",
    "                    if i + 1 < len(mask) and mask[i + 1] == 0:\n",
    "                        label.append(tkns[i + 1])\n",
    "                    else:\n",
    "                        label.append(\"[NOI]\")\n",
    "            tkns = train\n",
    "            score_arr = new_score\n",
    "            training_data.append((train, label))\n",
    "    noi_ratio[index] = Counter(training_data[-1][1])['[NOI]'] / len(training_data[-1][1])\n",
    "    \n",
    "with open(\"CoNLL-2003_InsertionTransformer\", \"wb\") as fout:\n",
    "    pk.dump(training_data, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8273283202800721"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noi_ratio.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POINTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39cb5a6d7f0f42f7801745046adf72e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1162.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def zero_runs(a):\n",
    "    # Create an array that is 1 where a is 0, and pad each end with an extra 0.\n",
    "    iszero = np.concatenate(([0], np.equal(a, 0).view(np.int8), [0]))\n",
    "    absdiff = np.abs(np.diff(iszero))\n",
    "    # Runs start and end where absdiff is 1.\n",
    "    ranges = np.where(absdiff == 1)[0].reshape(-1, 2)\n",
    "    return ranges\n",
    "\n",
    "# rqm only\n",
    "training_data = []\n",
    "noi_ratio = np.zeros(shape=len(result))\n",
    "for index, r in tqdm(enumerate(result), total=len(result)):\n",
    "    score_arr = np.zeros(len(r['tokenize']))\n",
    "    masked_span = np.zeros(len(r['tokenize']))\n",
    "    masked_span[0], masked_span[-1] = 2, 2\n",
    "    if len(r['tokenize']) > 512:\n",
    "        continue\n",
    "    for bpe_index, bpe in enumerate(r['tokenize']):\n",
    "        score = 0\n",
    "        # skill and special token\n",
    "        # words other than skills and start token\n",
    "        added = False\n",
    "        for pos_tag in [\"NN\", \"JJ\", \"VB\"]:\n",
    "            if pos_tag in bpe[1]:\n",
    "                score += 1\n",
    "                added = True\n",
    "                break\n",
    "        if not added:\n",
    "            score += 0.5\n",
    "        # tf-idf score\n",
    "        score += bpe[4]\n",
    "        # yake score\n",
    "        score += bpe[5]\n",
    "        # Turn score into negative\n",
    "        score_arr[bpe_index] = 4-score\n",
    "    rqm_tkn = [t[0] for t in r['tokenize']]\n",
    "    training_data.append((rqm_tkn, [\"[NOI]\"] * len(rqm_tkn)))\n",
    "    initial_length = len(r['tokenize'])\n",
    "    while len(rqm_tkn) > initial_length // 7:\n",
    "        zero_ranges = zero_runs(masked_span)\n",
    "        for span in zero_ranges:\n",
    "            start, end = span[0], span[1]\n",
    "            if span[1] - span[0] == 1:\n",
    "                masked_span[start] = 1\n",
    "            elif span[1] - span[0] == 2:\n",
    "                if score_arr[start] > score_arr[end-1]:\n",
    "                    masked_span[start] = 1\n",
    "                else:\n",
    "                    masked_span[end-1] = 1\n",
    "            else:\n",
    "                value, path = s.rob(score_arr[start:end])\n",
    "                for p in path:\n",
    "                    masked_span[start+p] = 1\n",
    "        if len(zero_ranges) == 0:\n",
    "            break\n",
    "        train, label = [], []\n",
    "        select_cursor = 0\n",
    "        for i in range(len(masked_span)):\n",
    "            if masked_span[i] != 1:\n",
    "                train.append(rqm_tkn[i])\n",
    "                if i + 1 < len(masked_span) and masked_span[i + 1] == 1:\n",
    "                    label.append(rqm_tkn[i+1])\n",
    "                    select_cursor += 1\n",
    "                else:\n",
    "                    label.append(\"[NOI]\")\n",
    "        training_data.append((train, label))\n",
    "        rqm_tkn = train\n",
    "        score_arr = score_arr[masked_span != 1]\n",
    "        masked_span = masked_span[masked_span != 1]\n",
    "        assert len(masked_span) == len(rqm_tkn) == len(score_arr)\n",
    "    noi_ratio[index] = Counter(training_data[-1][1])['[NOI]'] / len(training_data[-1][1])\n",
    "    \n",
    "with open(\"CoNLL-2003_InsertionTransformer_pointer\", \"wb\") as fout:\n",
    "    pk.dump(training_data, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POINTER enum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6711562a85b34ffa87b9890b4ed1c06c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1162.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def zero_runs(a):\n",
    "    # Create an array that is 1 where a is 0, and pad each end with an extra 0.\n",
    "    iszero = np.concatenate(([0], np.equal(a, 0).view(np.int8), [0]))\n",
    "    absdiff = np.abs(np.diff(iszero))\n",
    "    # Runs start and end where absdiff is 1.\n",
    "    ranges = np.where(absdiff == 1)[0].reshape(-1, 2)\n",
    "    return ranges\n",
    "\n",
    "# rqm only\n",
    "training_data = []\n",
    "noi_ratio = np.zeros(shape=len(result))\n",
    "for index, r in tqdm(enumerate(result), total=len(result)):\n",
    "    score_arr = np.zeros(len(r['tokenize']))\n",
    "    masked_span = np.zeros(len(r['tokenize']))\n",
    "    masked_span[0], masked_span[-1] = 2, 2\n",
    "    if len(r['tokenize']) > 512:\n",
    "        continue\n",
    "    for bpe_index, bpe in enumerate(r['tokenize']):\n",
    "        score = 0\n",
    "        # skill and special token\n",
    "        # words other than skills and start token\n",
    "        added = False\n",
    "        for pos_tag in [\"NN\", \"JJ\", \"VB\"]:\n",
    "            if pos_tag in bpe[1]:\n",
    "                score += 1\n",
    "                added = True\n",
    "                break\n",
    "        if not added:\n",
    "            score += 0.5\n",
    "        # tf-idf score\n",
    "        score += bpe[4]\n",
    "        # yake score\n",
    "        score += bpe[5]\n",
    "        # Turn score into negative\n",
    "        score_arr[bpe_index] = 4-score\n",
    "    rqm_tkn = [t[0] for t in r['tokenize']]\n",
    "    training_data.append((rqm_tkn, [\"[NOI]\"] * len(rqm_tkn)))\n",
    "    initial_length = 0\n",
    "    for skl in r['entities']:\n",
    "        initial_length += len(tokenizer.tokenize(' '.join(skl)))\n",
    "    while len(rqm_tkn) > initial_length:\n",
    "        zero_ranges = zero_runs(masked_span)\n",
    "        for span in zero_ranges:\n",
    "            start, end = span[0], span[1]\n",
    "            if span[1] - span[0] == 1:\n",
    "                masked_span[start] = 1\n",
    "            elif span[1] - span[0] == 2:\n",
    "                if score_arr[start] > score_arr[end-1]:\n",
    "                    masked_span[start] = 1\n",
    "                else:\n",
    "                    masked_span[end-1] = 1\n",
    "            else:\n",
    "                value, path = s.rob(score_arr[start:end])\n",
    "                for p in path:\n",
    "                    masked_span[start+p] = 1\n",
    "        if len(zero_ranges) == 0:\n",
    "            break\n",
    "        train, label = [], []\n",
    "        select_cursor = 0\n",
    "        for i in range(len(masked_span)):\n",
    "            if masked_span[i] != 1:\n",
    "                train.append(rqm_tkn[i])\n",
    "                if i + 1 < len(masked_span) and masked_span[i + 1] == 1:\n",
    "                    label.append(rqm_tkn[i+1])\n",
    "                    select_cursor += 1\n",
    "                else:\n",
    "                    label.append(\"[NOI]\")e\n",
    "        training_data.append((train, label))\n",
    "        rqm_tkn = train\n",
    "        score_arr = score_arr[masked_span != 1]\n",
    "        masked_span = masked_span[masked_span != 1]\n",
    "        assert len(masked_span) == len(rqm_tkn) == len(score_arr)\n",
    "    noi_ratio[index] = Counter(training_data[-1][1])['[NOI]'] / len(training_data[-1][1])\n",
    "    \n",
    "with open(\"CoNLL-2003_InsertionTransformer_pointer_enum\", \"wb\") as fout:\n",
    "    pk.dump(training_data, fout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bottom up wired algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deprecated because wired algo setting\n",
    "\n",
    "# rqm only\n",
    "training_data = []\n",
    "noi_ratio = np.zeros(shape=len(result))\n",
    "for index, r in tqdm(enumerate(result), total=len(result)):\n",
    "    score_arr = []\n",
    "    masked_span = []\n",
    "    if len(r['tokenize']) > 512:\n",
    "        continue\n",
    "    for bpe in r['tokenize']:\n",
    "        score, mask_value = 0, 0\n",
    "        # POS tag score\n",
    "        # Skill and special token\n",
    "        if bpe[3] != 'O':\n",
    "            score = 4\n",
    "            mask_value = 1\n",
    "        else:\n",
    "            # words other than skills and start token\n",
    "            added = False\n",
    "            for pos_tag in [\"NN\", \"JJ\", \"VB\"]:\n",
    "                if pos_tag in bpe[1]:\n",
    "                    score += 1\n",
    "                    added = True\n",
    "                    break\n",
    "            if not added:\n",
    "                score += 0.5\n",
    "            # tf-idf score\n",
    "            score += bpe[4]\n",
    "            # yake score\n",
    "            score += bpe[5]\n",
    "            mask_value = 0\n",
    "        # Turn score into negative\n",
    "        score_arr.append(score)\n",
    "        masked_span.append(mask_value)\n",
    "    tkns = [t[0] for t in r['tokenize']]\n",
    "    # Record number of NOI\n",
    "    recorded = False\n",
    "    while not all(masked_span):\n",
    "        cursor = 0\n",
    "        start, end = None, None\n",
    "        max_reward, max_reward_idx = float('-inf'), None\n",
    "        insert_index = []\n",
    "        while cursor < len(masked_span):\n",
    "            if masked_span[cursor] == 0:\n",
    "                if start is None:\n",
    "                    start = cursor\n",
    "                    end = cursor\n",
    "                else:\n",
    "                    end = cursor\n",
    "                    if (end - start > 0 and (end + 1) < len(masked_span) and masked_span[end + 1] != 1 \n",
    "                        and score_arr[cursor] > max_reward):\n",
    "                        max_reward = score_arr[cursor]\n",
    "                        max_reward_idx = cursor\n",
    "            elif end is not None:\n",
    "                if end - start + 1 > 2:\n",
    "                    insert_index.append(max_reward_idx)\n",
    "                else:\n",
    "                    if score_arr[start] > score_arr[end]:\n",
    "                        insert_index.append(start)\n",
    "                    else:\n",
    "                        insert_index.append(end)\n",
    "                # Clear span\n",
    "                start, end = None, None\n",
    "                max_reward, max_reward_idx = float('-inf'), None\n",
    "            cursor += 1\n",
    "        train, label = [], []\n",
    "        select_cursor = 0\n",
    "        for i, m, r in zip(range(len(masked_span)), masked_span, tkns):\n",
    "            if m == 1:\n",
    "                train.append(r)\n",
    "                if i + 1 < len(masked_span) and masked_span[i + 1] == 0:\n",
    "                    label.append(tkns[insert_index[select_cursor]])\n",
    "                    select_cursor += 1\n",
    "                else:\n",
    "                    label.append(\"[NOI]\")\n",
    "        training_data.append((train, label))\n",
    "        if not recorded:\n",
    "            noi_ratio[index] = Counter(training_data[-1][1])['[NOI]'] / len(training_data[-1][1])\n",
    "            recorded = True\n",
    "        for i_idx in insert_index:\n",
    "            masked_span[i_idx] = 1\n",
    "    training_data.append((tkns, [\"[NOI]\"] * len(tkns)))\n",
    "\n",
    "with open(\"CoNLL-2003_InsertionTransformer_bottom_up\", \"wb\") as fout:\n",
    "    pk.dump(training_data, fout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bottom up softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deprecared\n",
    "# Use softmax as one of the scores\n",
    "\n",
    "def generate_distance(start, end):\n",
    "    left_bound, right_bound = start - 1, end + 1\n",
    "    distance = [min(i - left_bound, right_bound - i) for i in range(start, end+1)]\n",
    "    return distance\n",
    "\n",
    "# rqm only\n",
    "training_data = []\n",
    "for r in tqdm(result):\n",
    "    score_arr = []\n",
    "    masked_span = []\n",
    "    if len(r['tokenize']) > 512:\n",
    "        continue\n",
    "    for bpe in r['tokenize']:\n",
    "        score, mask_value = 0, 0\n",
    "        # POS tag score\n",
    "        # Skill and special token\n",
    "        if bpe[3] != 'O':\n",
    "            score = 4\n",
    "            mask_value = 1\n",
    "        else:\n",
    "            # words other than skills and start token\n",
    "            added = False\n",
    "            for pos_tag in [\"NN\", \"JJ\", \"VB\"]:\n",
    "                if pos_tag in bpe[1]:\n",
    "                    score += 1\n",
    "                    added = True\n",
    "                    break\n",
    "            if not added:\n",
    "                score += 0.5\n",
    "            # tf-idf score\n",
    "            score += bpe[4]\n",
    "            # yake score\n",
    "            score += bpe[5]\n",
    "            mask_value = 0\n",
    "        # Turn score into negative\n",
    "        score_arr.append(score)\n",
    "        masked_span.append(mask_value)\n",
    "    score_arr = np.array(score_arr)\n",
    "    tkns = [t[0] for t in r['tokenize']]\n",
    "    while not all(masked_span):\n",
    "        cursor = 0\n",
    "        start, end = None, None\n",
    "        max_reward, max_reward_idx = float('-inf'), None\n",
    "        insert_index = []\n",
    "        while cursor < len(masked_span):\n",
    "            if masked_span[cursor] == 0:\n",
    "                if start is None:\n",
    "                    start = cursor\n",
    "                    end = cursor\n",
    "                else:\n",
    "                    end = cursor\n",
    "            elif end is not None:\n",
    "                overall_score = score_arr[start:end+1]\n",
    "                softmax_score = softmax(generate_distance(start, end))\n",
    "                if softmax_score.max() - softmax_score.min() == 0:\n",
    "                    overall_score += softmax_score\n",
    "                else:\n",
    "                    overall_score += (softmax_score - softmax_score.min()) / (softmax_score.max() - softmax_score.min())\n",
    "                insert_index.append(start + overall_score.argmax())\n",
    "                # Clear span\n",
    "                start, end = None, None\n",
    "            cursor += 1\n",
    "        train, label = [], []\n",
    "        select_cursor = 0\n",
    "        for i, m, r in zip(range(len(masked_span)), masked_span, tkns):\n",
    "            if m == 1:\n",
    "                train.append(r)\n",
    "                if i + 1 < len(masked_span) and masked_span[i + 1] == 0:\n",
    "                    label.append(tkns[insert_index[select_cursor]])\n",
    "                    select_cursor += 1\n",
    "                else:\n",
    "                    label.append(\"[NOI]\")\n",
    "        training_data.append((train, label))\n",
    "        for i_idx in insert_index:\n",
    "            masked_span[i_idx] = 1\n",
    "    training_data.append((tkns, [\"[NOI]\"] * len(tkns)))\n",
    "    \n",
    "with open(\"CoNLL-2003_InsertionTransformer_bottom_up_softmax\", \"wb\") as fout:\n",
    "    pk.dump(training_data, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b89ee0b988a543a48cfccb936db304bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1162.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Use softmax as masks\n",
    "def generate_distance(start, end):\n",
    "    left_bound, right_bound = start - 1, end + 1\n",
    "    distance = [min(i - left_bound, right_bound - i) for i in range(start, end+1)]\n",
    "    return distance\n",
    "\n",
    "# rqm only\n",
    "# Use softmax as a mask (multiply)\n",
    "training_data = []\n",
    "for r in tqdm(result):\n",
    "    score_arr = []\n",
    "    masked_span = []\n",
    "    if len(r['tokenize']) > 512:\n",
    "        continue\n",
    "    for bpe in r['tokenize']:\n",
    "        score, mask_value = 0, 0\n",
    "        # POS tag score\n",
    "        # Skill and special token\n",
    "        if bpe[3] != 'O':\n",
    "            score = 4\n",
    "            mask_value = 1\n",
    "        else:\n",
    "            # words other than skills and start token\n",
    "            added = False\n",
    "            for pos_tag in [\"NN\", \"JJ\", \"VB\"]:\n",
    "                if pos_tag in bpe[1]:\n",
    "                    score += 1\n",
    "                    added = True\n",
    "                    break\n",
    "            if not added:\n",
    "                score += 0.5\n",
    "            # tf-idf score\n",
    "            score += bpe[4]\n",
    "            # yake score\n",
    "            score += bpe[5]\n",
    "            mask_value = 0\n",
    "        # Turn score into negative\n",
    "        score_arr.append(score)\n",
    "        masked_span.append(mask_value)\n",
    "    score_arr = np.array(score_arr)\n",
    "    tkns = [t[0] for t in r['tokenize']]\n",
    "    while not all(masked_span):\n",
    "        cursor = 0\n",
    "        start, end = None, None\n",
    "        max_reward, max_reward_idx = float('-inf'), None\n",
    "        insert_index = []\n",
    "        while cursor < len(masked_span):\n",
    "            if masked_span[cursor] == 0:\n",
    "                if start is None:\n",
    "                    start = cursor\n",
    "                    end = cursor\n",
    "                else:\n",
    "                    end = cursor\n",
    "            elif end is not None:\n",
    "                overall_score = score_arr[start:end+1]\n",
    "                softmax_score = softmax(generate_distance(start, end))\n",
    "                if softmax_score.max() - softmax_score.min() != 0:\n",
    "                    overall_score *= (softmax_score - softmax_score.min()) / (softmax_score.max() - softmax_score.min())\n",
    "                insert_index.append(start + overall_score.argmax())\n",
    "                # Clear span\n",
    "                start, end = None, None\n",
    "            cursor += 1\n",
    "        train, label = [], []\n",
    "        select_cursor = 0\n",
    "        for i, m, r in zip(range(len(masked_span)), masked_span, tkns):\n",
    "            if m == 1:\n",
    "                train.append(r)\n",
    "                if i + 1 < len(masked_span) and masked_span[i + 1] == 0:\n",
    "                    label.append(tkns[insert_index[select_cursor]])\n",
    "                    select_cursor += 1\n",
    "                else:\n",
    "                    label.append(\"[NOI]\")\n",
    "        training_data.append((train, label))\n",
    "        for i_idx in insert_index:\n",
    "            masked_span[i_idx] = 1\n",
    "    training_data.append((tkns, [\"[NOI]\"] * len(tkns)))\n",
    "    \n",
    "with open(\"CoNLL-2003_InsertionTransformer_bottom_up_softmax_multiply\", \"wb\") as fout:\n",
    "    pk.dump(training_data, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8492"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bottom up softmax example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3381c823a33b463f98360091cbd94074",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1162.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def generate_distance(start, end):\n",
    "    left_bound, right_bound = start - 1, end + 1\n",
    "    distance = [min(i - left_bound, right_bound - i) for i in range(start, end+1)]\n",
    "    return distance\n",
    "\n",
    "# rqm only\n",
    "# Use softmax as a mask (multiply)\n",
    "example_arr = []\n",
    "for r in tqdm(result):\n",
    "    training_data = []\n",
    "    score_arr = []\n",
    "    masked_span = []\n",
    "    if len(r['tokenize']) > 512:\n",
    "        continue\n",
    "    for bpe in r['tokenize']:\n",
    "        score, mask_value = 0, 0\n",
    "        # POS tag score\n",
    "        # Skill and special token\n",
    "        if bpe[3] != 'O':\n",
    "            score = 4\n",
    "            mask_value = 1\n",
    "        else:\n",
    "            # words other than skills and start token\n",
    "            added = False\n",
    "            for pos_tag in [\"NN\", \"JJ\", \"VB\"]:\n",
    "                if pos_tag in bpe[1]:\n",
    "                    score += 1\n",
    "                    added = True\n",
    "                    break\n",
    "            if not added:\n",
    "                score += 0.5\n",
    "            # tf-idf score\n",
    "            score += bpe[4]\n",
    "            # yake score\n",
    "            score += bpe[5]\n",
    "            mask_value = 0\n",
    "        # Turn score into negative\n",
    "        score_arr.append(score)\n",
    "        masked_span.append(mask_value)\n",
    "    score_arr = np.array(score_arr)\n",
    "    tkns = [t[0] for t in r['tokenize']]\n",
    "    while not all(masked_span):\n",
    "        cursor = 0\n",
    "        start, end = None, None\n",
    "        max_reward, max_reward_idx = float('-inf'), None\n",
    "        insert_index = []\n",
    "        while cursor < len(masked_span):\n",
    "            if masked_span[cursor] == 0:\n",
    "                if start is None:\n",
    "                    start = cursor\n",
    "                    end = cursor\n",
    "                else:\n",
    "                    end = cursor\n",
    "            elif end is not None:\n",
    "                overall_score = score_arr[start:end+1]\n",
    "                softmax_score = softmax(generate_distance(start, end))\n",
    "                if softmax_score.max() - softmax_score.min() != 0:\n",
    "                    overall_score *= (softmax_score - softmax_score.min()) / (softmax_score.max() - softmax_score.min())\n",
    "                insert_index.append(start + overall_score.argmax())\n",
    "                # Clear span\n",
    "                start, end = None, None\n",
    "            cursor += 1\n",
    "        train, label = [], []\n",
    "        select_cursor = 0\n",
    "        for i, m, r in zip(range(len(masked_span)), masked_span, tkns):\n",
    "            if m == 1:\n",
    "                train.append(r)\n",
    "                if i + 1 < len(masked_span) and masked_span[i + 1] == 0:\n",
    "                    label.append(tkns[insert_index[select_cursor]])\n",
    "                    select_cursor += 1\n",
    "                else:\n",
    "                    label.append(\"[NOI]\")\n",
    "        training_data.append((train, label))\n",
    "        for i_idx in insert_index:\n",
    "            masked_span[i_idx] = 1\n",
    "    training_data.append((tkns, [\"[NOI]\"] * len(tkns)))\n",
    "    example_arr.append(training_data)\n",
    "    \n",
    "with open(\"CoNLL-2003_InsertionTransformer_bottom_up_softmax_multiply_example\", \"wb\") as fout:\n",
    "    pk.dump(example_arr, fout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bottom up GGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ccb86b85aea41078695577627a39d0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1162.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import gennorm\n",
    "\n",
    "# Use ggd as one of the scores\n",
    "\n",
    "beta = 8\n",
    "\n",
    "# rqm only\n",
    "training_data = []\n",
    "for r in tqdm(result):\n",
    "    score_arr = []\n",
    "    masked_span = []\n",
    "    if len(r['tokenize']) > 512:\n",
    "        continue\n",
    "    for bpe in r['tokenize']:\n",
    "        score, mask_value = 0, 0\n",
    "        # POS tag score\n",
    "        # Skill and special token\n",
    "        if bpe[3] != 'O':\n",
    "            score = 4\n",
    "            mask_value = 1\n",
    "        else:\n",
    "            # words other than skills and start token\n",
    "            added = False\n",
    "            for pos_tag in [\"NN\", \"JJ\", \"VB\"]:\n",
    "                if pos_tag in bpe[1]:\n",
    "                    score += 1\n",
    "                    added = True\n",
    "                    break\n",
    "            if not added:\n",
    "                score += 0.5\n",
    "            # tf-idf score\n",
    "            score += bpe[4]\n",
    "            # yake score\n",
    "            score += bpe[5]\n",
    "            mask_value = 0\n",
    "        # Turn score into negative\n",
    "        score_arr.append(score)\n",
    "        masked_span.append(mask_value)\n",
    "    score_arr = np.array(score_arr)\n",
    "    tkns = [t[0] for t in r['tokenize']]\n",
    "    while not all(masked_span):\n",
    "        cursor = 0\n",
    "        start, end = None, None\n",
    "        max_reward, max_reward_idx = float('-inf'), None\n",
    "        insert_index = []\n",
    "        while cursor < len(masked_span):\n",
    "            if masked_span[cursor] == 0:\n",
    "                if start is None:\n",
    "                    start = cursor\n",
    "                    end = cursor\n",
    "                else:\n",
    "                    end = cursor\n",
    "            elif end is not None:\n",
    "                overall_score = score_arr[start:end+1]\n",
    "                ggd_score = gennorm.pdf(np.linspace(gennorm.ppf(0.01, beta),\n",
    "                                                    gennorm.ppf(0.99, beta), end - start + 1),\n",
    "                                        beta)\n",
    "                if ggd_score.max() - ggd_score.min() == 0:\n",
    "                    overall_score += np.ones(shape=ggd_score.shape)\n",
    "                else:\n",
    "                    overall_score += (ggd_score - ggd_score.min()) / (ggd_score.max() - ggd_score.min())\n",
    "                insert_index.append(start + overall_score.argmax())\n",
    "                # Clear span\n",
    "                start, end = None, None\n",
    "            cursor += 1\n",
    "        train, label = [], []\n",
    "        select_cursor = 0\n",
    "        for i, m, r in zip(range(len(masked_span)), masked_span, tkns):\n",
    "            if m == 1:\n",
    "                train.append(r)\n",
    "                if i + 1 < len(masked_span) and masked_span[i + 1] == 0:\n",
    "                    label.append(tkns[insert_index[select_cursor]])\n",
    "                    select_cursor += 1\n",
    "                else:\n",
    "                    label.append(\"[NOI]\")\n",
    "        training_data.append((train, label))\n",
    "        for i_idx in insert_index:\n",
    "            masked_span[i_idx] = 1\n",
    "    training_data.append((tkns, [\"[NOI]\"] * len(tkns)))\n",
    "    \n",
    "with open(\"CoNLL-2003_InsertionTransformer_bottom_up_ggd\", \"wb\") as fout:\n",
    "    pk.dump(training_data, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import gennorm\n",
    "\n",
    "\n",
    "# Use GGD as masks\n",
    "beta = 8\n",
    "\n",
    "# rqm only\n",
    "training_data = []\n",
    "for r in tqdm(result):\n",
    "    score_arr = []\n",
    "    masked_span = []\n",
    "    if len(r['tokenize']) > 512:\n",
    "        continue\n",
    "    for bpe in r['tokenize']:\n",
    "        score, mask_value = 0, 0\n",
    "        # POS tag score\n",
    "        # Skill and special token\n",
    "        if bpe[3] != 'O':\n",
    "            score = 4\n",
    "            mask_value = 1\n",
    "        else:\n",
    "            # words other than skills and start token\n",
    "            added = False\n",
    "            for pos_tag in [\"NN\", \"JJ\", \"VB\"]:\n",
    "                if pos_tag in bpe[1]:\n",
    "                    score += 1\n",
    "                    added = True\n",
    "                    break\n",
    "            if not added:\n",
    "                score += 0.5\n",
    "            # tf-idf score\n",
    "            score += bpe[4]\n",
    "            # yake score\n",
    "            score += bpe[5]\n",
    "            mask_value = 0\n",
    "        # Turn score into negative\n",
    "        score_arr.append(score)\n",
    "        masked_span.append(mask_value)\n",
    "    score_arr = np.array(score_arr)\n",
    "    tkns = [t[0] for t in r['tokenize']]\n",
    "    while not all(masked_span):\n",
    "        cursor = 0\n",
    "        start, end = None, None\n",
    "        max_reward, max_reward_idx = float('-inf'), None\n",
    "        insert_index = []\n",
    "        while cursor < len(masked_span):\n",
    "            if masked_span[cursor] == 0:\n",
    "                if start is None:\n",
    "                    start = cursor\n",
    "                    end = cursor\n",
    "                else:\n",
    "                    end = cursor\n",
    "            elif end is not None:\n",
    "                overall_score = score_arr[start:end+1]\n",
    "                ggd_score = gennorm.pdf(np.linspace(gennorm.ppf(0.01, beta),\n",
    "                                                    gennorm.ppf(0.99, beta), end - start + 1),\n",
    "                                        beta)\n",
    "                if ggd_score.max() - ggd_score.min() != 0:\n",
    "                    overall_score *= (ggd_score - ggd_score.min()) / (ggd_score.max() - ggd_score.min())\n",
    "                insert_index.append(start + overall_score.argmax())\n",
    "                # Clear span\n",
    "                start, end = None, None\n",
    "            cursor += 1\n",
    "        train, label = [], []\n",
    "        select_cursor = 0\n",
    "        for i, m, r in zip(range(len(masked_span)), masked_span, tkns):\n",
    "            if m == 1:\n",
    "                train.append(r)\n",
    "                if i + 1 < len(masked_span) and masked_span[i + 1] == 0:\n",
    "                    label.append(tkns[insert_index[select_cursor]])\n",
    "                    select_cursor += 1\n",
    "                else:\n",
    "                    label.append(\"[NOI]\")\n",
    "        training_data.append((train, label))\n",
    "        for i_idx in insert_index:\n",
    "            masked_span[i_idx] = 1\n",
    "    training_data.append((tkns, [\"[NOI]\"] * len(tkns)))\n",
    "    \n",
    "with open(\"CoNLL-2003_InsertionTransformer_bottom_up_ggd_multiply\", \"wb\") as fout:\n",
    "    pk.dump(training_data, fout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bottom up only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "622b07ae62e6455388170322dd789902",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1162.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# rqm only\n",
    "# Use softmax as a mask (multiply)\n",
    "training_data = []\n",
    "for r in tqdm(result):\n",
    "    score_arr = []\n",
    "    masked_span = []\n",
    "    if len(r['tokenize']) > 512:\n",
    "        continue\n",
    "    for bpe in r['tokenize']:\n",
    "        score, mask_value = 0, 0\n",
    "        # POS tag score\n",
    "        # Skill and special token\n",
    "        if bpe[3] != 'O':\n",
    "            score = 4\n",
    "            mask_value = 1\n",
    "        else:\n",
    "            # words other than skills and start token\n",
    "            added = False\n",
    "            for pos_tag in [\"NN\", \"JJ\", \"VB\"]:\n",
    "                if pos_tag in bpe[1]:\n",
    "                    score += 1\n",
    "                    added = True\n",
    "                    break\n",
    "            if not added:\n",
    "                score += 0.5\n",
    "            # tf-idf score\n",
    "            score += bpe[4]\n",
    "            # yake score\n",
    "            score += bpe[5]\n",
    "            mask_value = 0\n",
    "        # Turn score into negative\n",
    "        score_arr.append(score)\n",
    "        masked_span.append(mask_value)\n",
    "    score_arr = np.array(score_arr)\n",
    "    tkns = [t[0] for t in r['tokenize']]\n",
    "    while not all(masked_span):\n",
    "        cursor = 0\n",
    "        start, end = None, None\n",
    "        max_reward, max_reward_idx = float('-inf'), None\n",
    "        insert_index = []\n",
    "        while cursor < len(masked_span):\n",
    "            if masked_span[cursor] == 0:\n",
    "                if start is None:\n",
    "                    start = cursor\n",
    "                    end = cursor\n",
    "                else:\n",
    "                    end = cursor\n",
    "            elif end is not None:\n",
    "                overall_score = score_arr[start:end+1]\n",
    "                insert_index.append(start + overall_score.argmax())\n",
    "                # Clear span\n",
    "                start, end = None, None\n",
    "            cursor += 1\n",
    "        train, label = [], []\n",
    "        select_cursor = 0\n",
    "        for i, m, r in zip(range(len(masked_span)), masked_span, tkns):\n",
    "            if m == 1:\n",
    "                train.append(r)\n",
    "                if i + 1 < len(masked_span) and masked_span[i + 1] == 0:\n",
    "                    label.append(tkns[insert_index[select_cursor]])\n",
    "                    select_cursor += 1\n",
    "                else:\n",
    "                    label.append(\"[NOI]\")\n",
    "        training_data.append((train, label))\n",
    "        for i_idx in insert_index:\n",
    "            masked_span[i_idx] = 1\n",
    "    training_data.append((tkns, [\"[NOI]\"] * len(tkns)))\n",
    "    \n",
    "with open(\"CoNLL-2003_InsertionTransformer_bottom_up_only\", \"wb\") as fout:\n",
    "    pk.dump(training_data, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17694"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noi_ratio.mean()\n",
    "len(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bottom up example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a2d2ea9dfb142a1bc9b1e34e6d33550",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1162.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# rqm only\n",
    "# Use softmax as a mask (multiply)\n",
    "example_arr = []\n",
    "for r in tqdm(result):\n",
    "    training_data = []\n",
    "    score_arr = []\n",
    "    masked_span = []\n",
    "    if len(r['tokenize']) > 512:\n",
    "        continue\n",
    "    for bpe in r['tokenize']:\n",
    "        score, mask_value = 0, 0\n",
    "        # POS tag score\n",
    "        # Skill and special token\n",
    "        if bpe[3] != 'O':\n",
    "            score = 4\n",
    "            mask_value = 1\n",
    "        else:\n",
    "            # words other than skills and start token\n",
    "            added = False\n",
    "            for pos_tag in [\"NN\", \"JJ\", \"VB\"]:\n",
    "                if pos_tag in bpe[1]:\n",
    "                    score += 1\n",
    "                    added = True\n",
    "                    break\n",
    "            if not added:\n",
    "                score += 0.5\n",
    "            # tf-idf score\n",
    "            score += bpe[4]\n",
    "            # yake score\n",
    "            score += bpe[5]\n",
    "            mask_value = 0\n",
    "        # Turn score into negative\n",
    "        score_arr.append(score)\n",
    "        masked_span.append(mask_value)\n",
    "    score_arr = np.array(score_arr)\n",
    "    tkns = [t[0] for t in r['tokenize']]\n",
    "    while not all(masked_span):\n",
    "        cursor = 0\n",
    "        start, end = None, None\n",
    "        max_reward, max_reward_idx = float('-inf'), None\n",
    "        insert_index = []\n",
    "        while cursor < len(masked_span):\n",
    "            if masked_span[cursor] == 0:\n",
    "                if start is None:\n",
    "                    start = cursor\n",
    "                    end = cursor\n",
    "                else:\n",
    "                    end = cursor\n",
    "            elif end is not None:\n",
    "                overall_score = score_arr[start:end+1]\n",
    "                insert_index.append(start + overall_score.argmax())\n",
    "                # Clear span\n",
    "                start, end = None, None\n",
    "            cursor += 1\n",
    "        train, label = [], []\n",
    "        select_cursor = 0\n",
    "        for i, m, r in zip(range(len(masked_span)), masked_span, tkns):\n",
    "            if m == 1:\n",
    "                train.append(r)\n",
    "                if i + 1 < len(masked_span) and masked_span[i + 1] == 0:\n",
    "                    label.append(tkns[insert_index[select_cursor]])\n",
    "                    select_cursor += 1\n",
    "                else:\n",
    "                    label.append(\"[NOI]\")\n",
    "        training_data.append((train, label))\n",
    "        for i_idx in insert_index:\n",
    "            masked_span[i_idx] = 1\n",
    "    training_data.append((tkns, [\"[NOI]\"] * len(tkns)))\n",
    "    example_arr.append(training_data)\n",
    "    \n",
    "with open(\"CoNLL-2003_InsertionTransformer_bottom_up_only_example\", \"wb\") as fout:\n",
    "    pk.dump(example_arr, fout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "586c785617a040088065aeb171478ee9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=231.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Prepare testing data for requirement insertion only\n",
    "\n",
    "with open(\"eng.testb\") as fin:\n",
    "    testing, lines = [], []\n",
    "    for line in fin.readlines():\n",
    "        line = line.split()\n",
    "        if len(line) > 0:\n",
    "            if line[1] == '-X-':\n",
    "                testing.append(lines)\n",
    "                lines = []\n",
    "            else:\n",
    "                lines.append(line)\n",
    "\n",
    "testing_data = []\n",
    "for test in tqdm(testing):\n",
    "    gt = ' '.join([line[0] for line in test])\n",
    "    content = ' '.join([line[0] for line in test if line[-1] != 'O'])\n",
    "    testing_data.append((\" [CLS] \" + content + \" [SEP] \", gt))\n",
    "    \n",
    "with open(\"CoNLL-2003_InsertionTransformer_test\", \"wb\") as fout:\n",
    "    pk.dump(testing_data, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a09e043afff4db39a6eb1374aef2f87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=231.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec214e22df9e42c5a6fbfaf7abefb3b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=231.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Prepare testing data for requirement only but with span inference the phrase\n",
    "\n",
    "with open(\"eng.testb\") as fin:\n",
    "    testing, lines = [], []\n",
    "    for line in fin.readlines():\n",
    "        line = line.split()\n",
    "        if len(line) > 0:\n",
    "            if line[1] == '-X-':\n",
    "                testing.append(lines)\n",
    "                lines = []\n",
    "            else:\n",
    "                lines.append(line)\n",
    "                \n",
    "entities = []\n",
    "for test in tqdm(testing):\n",
    "    entity = []\n",
    "    tmp_arr = []\n",
    "    last_tag = ''\n",
    "    for line in test:\n",
    "        if len(tmp_arr) != 0 and last_tag != line[3]:\n",
    "            entity.append(tmp_arr)\n",
    "            tmp_arr = []\n",
    "        if line[3] != 'O':\n",
    "            tmp_arr.append(line[0])\n",
    "        last_tag = line[3]\n",
    "    entities.append(entity)\n",
    "\n",
    "testing_data = []\n",
    "for test, entity in tqdm(zip(testing, entities), total=len(testing)):\n",
    "    gt = ' '.join([line[0] for line in test])\n",
    "    out_str = []\n",
    "    phrase_num = []\n",
    "    for i, e in enumerate(entity):\n",
    "        tokens = tokenizer.tokenize(' '.join(e))\n",
    "        out_str.append(' '.join(e))\n",
    "        phrase_num += [i] * len(tokens)\n",
    "    # add begin and end token \n",
    "    phrase_num = [-1] + phrase_num + [-1]\n",
    "    testing_data.append((\" [CLS] \" + \" \".join(out_str) + \" [SEP] \", gt, phrase_num))\n",
    "    \n",
    "with open(\"CoNLL-2003_InsertionTransformer_test_span_inf\", \"wb\") as fout:\n",
    "    pk.dump(testing_data, fout)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
